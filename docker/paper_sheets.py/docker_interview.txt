Basic Docker Interview Questions
First, familiarize yourself with some fundamental Docker concepts. These basic questions will help you build your understanding and prepare for the initial phase of the interview. 

1. What is a Docker image?
A Docker image is like a blueprint that creates containers. It has everything a developer needs to run an application, such as: 

Code 
Libraries 
Settings 
When you use a Docker image, Docker turns it into a container, which is a fully isolated environment. That's where the application runs independently. 

2. What is a Docker host? 
A Docker host is the system where we install Docker. It acts as the primary environment that runs and manages Docker containers. We can set up a Docker host on a local device or in a virtual or cloud environment.

3. How is a Docker client different from a Docker daemon? Can you share an example? 
The Docker client and Docker daemon work side-by-side but have separate roles. The Docker client is the tool that sends commands and the Docker daemon is the engine that acts on those commands. 

For example, if we type the docker run command to start a container, the client will take the request and send it to the Docker daemon. The Docker daemon will then handle the real work by starting the container. 

4. Can you explain what Docker networking is and which commands can create a bridge and an overlay network?
Docker networking allows containers to connect and communicate with other containers and hosts. The docker network create command allows us to set up user-defined networks.  

Bridge network: Creates a local network for communication between containers on the same Docker host.
Command: docker network create -d bridge my-bridge-network
This sets up a bridge network called my-bridge-network for containers on the same host.
Overlay network: Enables communication between containers across multiple Docker hosts, often used in a Swarm setup.
Command: docker network create --scope=swarm --attachable -d overlay my-multihost-network
This creates an attachable overlay network called my-multihost-network for containers running on different hosts in a Docker Swarm.
5. Explain how Docker bridge networking works. 
Bridge networking is the default setup Docker uses to connect containers. If we don't specify a network, Docker links it to the bridge network. This bridge connects all containers on the same Docker host. Each container has a unique IP address, which allows containers to communicate directly. 

Intermediate Docker Interview Questions
These questions are asked to test your knowledge of intermediate-level Docker concepts.

6. What is a Dockerfile? Explain how you would write it. 
A Dockerfile is a script that defines instructions for building a Docker image. Each command in the Dockerfile sets up a specific part of the environment. When we run these commands, Docker builds an image layer-by-layer. Here's how we can write it: 

First, choose a base image. It contains essential tools for the app. 
Next, specify a working directory inside the container. That's where app files will be stored and run. 
In the third step, use COPY . . command to copy all project files into the container's working directory. 
Use the RUN command to install dependencies. 
Use the EXPOSE command to specify the port on which your app runs. 
Now, define the command Docker should run when it starts the container. 
Here’s a simple example of a Dockerfile for a Python web application:


# Step 1: Choose a base image
FROM python:3.9-slim
# Step 2: Specify the working directory
WORKDIR /app
# Step 3: Copy project files into the container
COPY . .
# Step 4: Install dependencies
RUN pip install -r requirements.txt
# Step 5: Expose the port the app runs on
EXPOSE 5000
# Step 6: Define the default command
CMD ["python", "app.py"]
Powered By 
Using the Dockerfile above, you can build an image with docker build -t my-python-app . and run a container using docker run -p 5000:5000 my-python-app.

7. What is Docker Compose, and how is it different from Dockerfile?
Docker Compose is a tool for defining and managing multi-container Docker applications using a YAML file (docker-compose.yml). It allows us to configure services, networks, and volumes in a single file, making it easier to manage complex applications.

Differences from Dockerfile:

A Dockerfile is used to build a single Docker image by defining its layers and dependencies.
Docker Compose is used to run and orchestrate multiple containers that may rely on each other (e.g., a web app container and a database container).
For example, a docker-compose.yml file might look like this:


version: '3.9'
services:
  web:
    build: .
    ports:
      - "5000:5000"
    depends_on:
      - db
  db:
    image: postgres
    volumes:
      - db-data:/var/lib/postgresql/data
volumes:
  db-data:
Powered By 
This file defines two services, web and db, with networking and volume configurations.

8. Why do we use volumes in Docker? 
We use Docker volumes to keep data safe outside of Docker containers. They provide a separate location on hosts where data lives even if the container gets removed. Also, it's easier to manage, back up, and share the volumes among containers. 

9. What are Docker bind mounts, and why do we prefer volumes over bind mounts? 
With Docker bind mounts, we can share files between the host machine and a container. They connect a specific file on the host system to a location in the container. If we make any changes to files, it'll instantly show up inside the container. 

Docker mounts are suitable for real-time file sharing — but they rely on the host OS, which raises security issues. 

On the contrary, since Docker volumes work independently, they're more secure than mounts.

Diagram of docker bind mounts and volumes

Diagram of docker bind mounts and volumes. Image source: Docker

10. What is Docker Swarm?
Docker Swarm is a container orchestration tool that manages and deploys services across a cluster of Docker nodes. It enables high availability, scalability, and load balancing, allowing multiple hosts to act as a single virtual Docker engine.

11. Can we autoscale Docker Swarm?
No, Docker Swarm does not natively support automatic autoscaling. To achieve autoscaling, we need to integrate monitoring tools and use scripts to manually adjust the number of instances. Here's how: 

Install a monitoring tool, like Prometheus or Grafana, to track resource usage, like CPU and memory.
Set the scaling triggers. For instance, we can define that CPU usage over 82% will trigger scaling up. 
Next, write a script with the docker service scale command to adjust the number of replicas. For example, to scale a service to 5 replicas: docker service scale <service_name>=5
By combining monitoring tools, triggers, and scripts, you can implement a form of autoscaling in Docker Swarm, even though it’s not built-in.

12. How would you use Docker Compose to scale services?
To scale services using Docker Compose, we can use the --scale flag with the docker-compose up command. This is typically used for stateless services like web servers. For example, to scale a web service to 3 instances:

docker-compose up --scale web=3

It’s important to ensure the docker-compose.yml file has properly defined services and uses an external load balancer or supports scaled instances. Scaling stateful services (e.g., databases) requires additional configuration to ensure data consistency.

13. Can a container restart by itself? Define its default and always policies. 
Yes, a container can restart by itself. However, we need to set a restart policy for that. 

Docker has different restart policies that control when and how containers should restart. The default policy is no, which means a container won't restart if it stops. With the always policy, Docker will automatically restart the container whenever it stops. 

We can use this command to apply always policy: 

docker run --restart=always <container-name>

Advanced Docker Interview Questions
Now, let’s move on to advanced Docker interview questions!

14. Explain the lifecycle of Docker containers. 
A Docker container passes through a lifecycle that defines the states a container can be in and how it operates in those states. The stages in a Docker container lifecycle are: 

Create: In this state, we set up a container from an image with the docker create command. 
Run: Here, we use the docker start command to run the container, which performs the tasks until we stop or pause it. 
Pause: We use the docker pause command to stop the process. This state keeps the memory and disk intact. If you want to resume the container, use the docker unpause command. 
Stop: If the container is inactive, it enters the stop stage, but this can happen due to multiple reasons: 
Immediate stop: The docker kill command stops the container without cleanup. 
Process completion: When the container finishes the task, it stops automatically. 
Out of memory: Container stops when it consumes too much memory. 
Delete: In the final stage, we remove the stopped or created container with the docker rm command.
15. What is a Docker image repository? 
A Docker image repository stores and shares multiple container images of the same name with the clients or the community. We can label them with tags to differentiate their different versions. For example, app/marketing_campaign:v1 will be the first version of a marketing app, and app/marketing_campaign:v2 will be the second version.

Docker Hub, the most popular Docker image repository, allows users to host, share, and pull container images publicly or privately. Other alternatives include Amazon ECR, Google Artifact Registry, and GitHub Container Registry.

16. Tell me about 3 best practices to keep a Docker container safe.
To enhance container security and minimize common vulnerabilities, I follow these best practices:

Choose lightweight images: Use minimal base images like Alpine to reduce the attack surface. 
Limit system calls: Since Docker containers can access unnecessary calls, use tools like Seccomp to restrict these calls. 
Secure sensitive data: Use Docker secrets to manage API keys or passwords. They encrypt the secrets and make them available only during runtime. 
17. Why do Docker containers need health checks? 
Docker containers rely on health checks to ensure they run smoothly. Deploying a container that is running but doesn't process requests can create issues for deployment teams. Health checks monitor these issues in real-time and informs us instantly.

For example, a health check can be added in a Dockerfile like this:

HEALTHCHECK --interval=30s --timeout=10s --retries=3 CMD curl -f http://localhost:8080/health || exit 1

This health check pings the container’s health endpoint every 30 seconds and marks the container as unhealthy if it fails three consecutive attempts. This proactive monitoring helps identify and resolve issues promptly.

18. What are dangling images in Docker, and how do you remove them? 
Dangling images in Docker are unused image layers that no longer have any tags associated with them. They often build up when you create new images with the same name and tag, leaving the old layers without references. These images can consume significant disk space, so it’s important to clean them up. Here's how: 

Run the docker images -f dangling=true command to find dangling images. 
Then, run the docker image prune -f command to delete all images in one go. 
If you want to remove images manually, use the docker rmi -f $(docker images -f dangling=true -q) command. 
These steps help keep your system clean and free up storage efficiently.

Docker and Kubernetes Interview Questions
Docker and Kubernetes are often used together, so finding some Kubernetes questions in a Docker interview wouldn't be unexpected, particularly if the role is oriented to DevOps. Here are some questions you may be asked:

19. What is the primary difference between Docker and Kubernetes?
Docker is a containerization platform that allows you to build, ship, and run containers. It focuses on creating and managing individual containers.Kubernetes, on the other hand, is an orchestration platform designed to manage multiple containers at scale. It handles deployment, scaling, load balancing, and self-healing across clusters of nodes.

Learn more about the differences in the Kubernetes vs Docker blog post.

20. Compare Docker Swarm with Kubernetes. 
Kubernetes and Docker Swarm manage containers, but they work differently:

Kubernetes manages large and complex container setups. Its self-healing and built-in monitoring features make it a more suitable option for complex environments.
Docker Swarm is suitable for smaller or less complex setups as it doesn't offer any built-in features like Kubernetes. We can easily integrate it with Docker tools like Docker CLI and Docker Compose.
21. How does Kubernetes manage a large number of Docker containers?
While Docker is great for creating and running containers, managing a large number of them requires Kubernetes. Kubernetes orchestrates containers efficiently by:

Setting resource limits: It allocates CPU, memory, and other resources to each container to prevent overconsumption.
Scheduling containers: Kubernetes decides where to run each container, optimizing resource utilization across nodes in a cluster.
Scaling automatically: Based on workload demand, it scales pods (groups of one or more containers) up or down to maintain performance and efficiency.
By automating these processes, Kubernetes ensures smooth operation, even when managing thousands of containers. While occasional errors can occur, its self-healing capabilities, like restarting failed containers, minimize disruptions.

22. What is a Pod in Kubernetes, and how is it different from a container?
A Pod is the smallest deployable unit in Kubernetes and represents a group of one or more containers that share the same network namespace, storage, and configuration.Unlike individual containers, Pods allow multiple tightly coupled containers to work together as a single unit (e.g., a web server and a sidecar logging container).

Overview of a Kubernetes node, highlighting pods and containers

Overview of a Kubernetes node, highlighting pods and containers. Image source: Kubernetes.

23. How can you manage sensitive data like passwords in Docker and Kubernetes?
In Docker: We can use Docker secrets, which encrypt sensitive data and make it accessible only to authorized containers at runtime.
In Kubernetes: We use Secrets objects, which store sensitive data like passwords, tokens, and API keys. Secrets can be mounted as volumes or exposed as environment variables to pods securely.
Example in Kubernetes:


apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  password: cGFzc3dvcmQ=  # Base64-encoded "password"
Powered By 
Scenario-Based Docker Interview Questions
The interviewer asks scenario-based and problem-solving questions to test how you approach real-world problems. Let's take a look at some questions to give you an idea: 

24. Imagine you're creating an image of a Maven-based API. You've already set up the Dockerfile with basic configurations. Now, you notice the image size is large. How would you reduce it? 
Example answer:

To reduce the size of a Docker image for a Maven-based API, I would follow these steps:

Create a .dockerignore file in the project directory to specify files and folders that should not be included in the Docker build context. This prevents unnecessary files from being added to the image, reducing its size. For example, I’d add the following to .dockerignore:


.git        # Version control files
target      # Compiled code and build artifacts
.idea       # IDE configuration files
Powered By 
Optimize the Dockerfile using multi-stage builds. I’d build the Maven project in one stage and copy only the necessary artifacts (e.g., compiled JAR files) into the final stage to keep the image small. Example Dockerfile with multi-stage build:


# Stage 1: Build the application
FROM maven:3.8.5-openjdk-11 AS build
WORKDIR /app
COPY pom.xml .
COPY src ./src
RUN mvn clean package
# Stage 2: Create a lightweight runtime image
FROM openjdk:11-jre-slim
WORKDIR /app
COPY --from=build /app/target/my-api.jar .
CMD ["java", "-jar", "my-api.jar"]
Powered By 
By ignoring unnecessary files and using multi-stage builds, one can significantly reduce the image size while maintaining efficiency.

25. Imagine you need to push a Docker container image to the Docker Hub with Jenkins. How would you do that?
Example answer:

Here's how I’d push a Docker container image to Docker Hub with Jenkins: 

Configure a Jenkins pipeline: Create a multi-branch pipeline job in Jenkins and link it to the repository containing the Dockerfile and Jenkinsfile.
Define the pipeline in my Jenkinsfile: The Jenkinsfile would include the following steps:
Build the Docker image
Log in to Docker Hub (using credentials stored securely in Jenkins)
Push the image to Docker Hub
Run the pipeline: Trigger the Jenkins job. It will build the image, log in to Docker Hub, and push the image automatically.
26. Imagine you have to migrate a WordPress Docker container to a new server without losing any data. How would you do that?
Example answer: 
Here’s how I’d migrate a WordPress Docker container:

Backup the WordPress data: Export the container’s persistent data (WordPress files and database). I’d use docker cp or a volume backup tool to back up the necessary volumes, typically the html directory for WordPress files and the database volume.
Transfer the backup files: I’d use scp to securely copy the backup files to the new server.
Set up WordPress on the new server: I’d deploy a new WordPress container and database container on the new server.
Restart and verify: Lastly, I’d restart the containers to apply the changes and verify that the WordPress site is running correctly.
By backing up volumes and restoring them to a new server, one can migrate WordPress without losing data. This method avoids reliance on specific extensions and provides more control over the migration process.

Tips for Preparing for a Docker Interview
If you’re reading this guide, you've already taken an important step to excel in your upcoming interview! But for complete beginners, preparing for an interview can be overwhelming. That's why I've put together some tips:

Master the basics of Docker
To excel in a Docker interview, start with a solid understanding of its core concepts. 

Learn how Docker images serve as the blueprint for containers, and practice creating, running, and managing containers to get familiar with their lightweight, isolated environments. 
Explore Docker volumes to handle persistent data effectively, and dive into networking by experimenting with bridge, host, and overlay networks to facilitate container communication. 
Study Dockerfiles to understand how images are built, focusing on key instructions like FROM, RUN, and CMD. 
Additionally, get hands-on with Docker Compose to manage multi-container applications and understand how Docker registries, such as Docker Hub, store and share images.
DataCamp offers plenty of other resources to guide you throughout your learning journey: 


"""
What is the difference between a Container and an Image?

A container is the environment in which the image will run, it stores environment
variables, has it's own endpoint and data.
"""

"""
What are the differences between Docker and VM?

The main difference is that Docker runs on top of the host kernel — this means 
Docker shares the same kernel as the host. On the other hand, a virtual machine 
emulates an entire operating system, which requires booting up a full OS and 
results in a slower startup process.

VMs are useful when you need full isolation between environments. A great 
example would be running untrusted applications, where strong separation from 
the host is critical.
"""

"""
7. What is Docker Compose, and how is it different from Dockerfile?
Docker Compose is a tool for defining and managing multi-container Docker 
applications using a YAML file (docker-compose.yml). It allows us to configure 
services, networks, and volumes in a single file, making it easier to manage 
complex applications.

Differences from Dockerfile:

A Dockerfile is used to build a single Docker image by defining its layers and 
dependencies. Docker Compose is used to run and orchestrate multiple containers 
that may rely on each other (e.g., a web app container and a database container).
"""

"""
8. Why do we use volumes in Docker? 
We use Docker volumes to keep data safe outside of Docker containers. They 
provide a separate location on hosts where data lives even if the container 
gets removed. Also, it's easier to manage, back up, and share the volumes among 
containers. 
"""

"""
What is the difference between docker run and docker start?

The docker run command is used to generate a new container from an image and
docker start is meant to start a container that was already created before
"""

"""
Question 1:
What is the difference between COPY and ADD commands on Dockerfile?

### 📚 Dockerfile: `COPY` vs `ADD`

Both `COPY` and `ADD` are instructions in a Dockerfile used to move files into a Docker image during build time. However, they have different purposes and behaviors.

---

#### ✅ `COPY` (Recommended for Most Use Cases)

`COPY` is a simple command that copies files and directories from the host (build context) into the Docker image.

**Syntax:**

```dockerfile
COPY <src> <dest>
```

**Example:**

```dockerfile
COPY requirements.txt /app/
COPY ./src/ /app/src/
```

✔️ Use `COPY` when you just need to copy files/folders into the container.

---

#### ⚠️ `ADD` (Has Additional Features)

`ADD` works similarly to `COPY`, but also supports:

* Automatically extracting local tar archives (e.g., `.tar`, `.tar.gz`)
* Downloading files from remote URLs

**Syntax:**

```dockerfile
ADD <src> <dest>
```

**Example 1 – Extracting a tar file:**

```dockerfile
ADD myapp.tar.gz /app/
```

> This will extract the contents of `myapp.tar.gz` into `/app/`.

**Example 2 – Downloading from a URL (not recommended):**

```dockerfile
ADD https://example.com/file.txt /app/file.txt
```

---

#### 🔐 Best Practices

* ✅ **Use `COPY`** for most tasks. It's predictable and secure.
* ❌ **Avoid `ADD`** for downloading files — use `curl` or `wget` inside `RUN` steps instead:

  ```dockerfile
  RUN curl -o /app/file.txt https://example.com/file.txt
  ```

"""

"""
Question 15:
What is a Docker image repository? 

A Docker image repository stores and shares multiple container images of the same name with 
the clients or the community. We can label them with tags to differentiate their different 
versions. For example, app/marketing_campaign:v1 will be the first version of a marketing app, 
and app/marketing_campaign:v2 will be the second version.

Docker Hub, the most popular Docker image repository, allows users to host, share, and pull 
container images publicly or privately. Other alternatives include Amazon ECR, Google Artifact Registry,
and GitHub Container Registry.
"""

"""
Question 7:
What is Docker Compose, and how is it different from Dockerfile?
Docker Compose is a tool for defining and managing multi-container Docker applications using a 
YAML file (docker-compose.yml). It allows us to configure services, networks, and volumes in a 
single  file, making it easier to manage complex applications.

Differences from Dockerfile:

- A Dockerfile is used to build a single Docker image by defining its layers and dependencies.
- Docker Compose is used to run and orchestrate multiple containers that may rely on each other 
(e.g., a web app container and a database container).
"""

"""
Docker Overview
Docker is an open-source platform for building, testing, deploying, updating, managing, 
and scaling applications. It streamlines operations by offering software packed into 
standardised and executable units called containers. These units comprise essential 
components required to run the software. Examples of components include runtime, 
libraries, system tools, and code.

Basic Docker Interview Questions
The fundamental level interview questions on Docker are as follows:

1. What is Docker, and why is it used?
Docker is an open-source platform for application development, from building to scaling. 
It is preferred for its ability to accelerate the process of building, sharing, and running 
applications without requiring environment configuration or management.

Docker integrates with existing software development tools, optimizing the workflow. The 
platform is also compatible with multiple cloud platforms.

2. What is a Docker container?
A Docker container is a standard software packaging unit that contains code and other 
dependencies such as system libraries, runtime, system tools, and settings. These speed 
up the application's running across various environments. In simple terms, Docker can be 
considered an executable, standalone, and lightweight package.

3. How do you create a Docker container?
A Docker container can be created using the ‘docker container create’ command in the 
system command. This command establishes the new container from the specified image 
without starting it immediately. The newly created container is generated in a ‘created’ 
state, writable, and open to running specific commands.

4. How does Docker differ from a virtual machine?
Docker is a software platform for creating and running Docker containers, while a Virtual 
Machine refers to the physical machine that runs an operating system.

Docker boots in a few seconds, uses an execution engine, requires less memory, and involves 
a complex usage method. VM takes a few minutes to boot, uses a hypervisor, is less memory 
efficient, and is easy to use.

5. What is a Docker image?
Docker images are read-only templates in the form of a blueprint or snapshot. They contain 
guidelines or instructions for creating a container. These images are standalone and 
executable files that comprise the dependencies, libraries, and files essential for the 
container.

Docker images are shareable and portable and can be stored in registries to track projects, 
user group access, complex software architectures, and others.

6. How do you push and pull Docker images?
Docker images are stored in Docker Hub. To obtain the image or pull it, the required command 
is
docker pull rocker/verse

To push the image to the newly created repository, the appropriate command is
docker push yourhubusername/verse_gapminder

7. What is a Dockerfile?
A Dockerfile is a text document comprising all the commands required for image assembly. 
Users call these commands. Examples of Dockerfile commands include ‘ADD’, ‘ARG’, ‘COPY’, 
‘LABEL’, and many more.

The commands aren’t case-sensitive, but being in uppercase helps easy identification of 
the arguments used in the command. Dockerfiles are used to run the instructions by Docker, 
and they must begin with a ‘FROM’ instruction.

8. What is a Docker registry?
The Docker registry is the centralized storage, management, and distribution system for 
container images. It comprises Docker repositories that contain different versions of the 
images. Images from the registry can be accessed through push and pull commands.

9. What is Docker Hub?
Docker Hub is a hosted registry solution available directly from Docker Inc. It provides 
public and private repositories and solutions like automated build and integrated source 
control.

The public repository is open for access, and the image names are as follows: 
organization/user name. Private repositories are private to the repository creator or 
organization member. The Docker Hub images are verified and secured for safe usage.

10. What is Docker Compose?
Docker Compose is a tool for defining and running multi-container applications. It allows 
projects to run with multiple containers from a single source: a YAML configuration file.

Docker Compose allows you to manage various services, data volumes, and networks easily. It 
is also compatible with working in all environments, such as production, staging, development, 
testing, and CI workflows.

Intermediate Docker Interview Questions
The Docker interview questions for the intermediate level are as follows:

11. How do you create a Docker network, and why would you need it?
A Docker network can be created through the following command:

docker network create [OPTIONS] NETWORK
The Docker network connects different services running in separate containers. Its 
functionality is essential, especially in scenarios requiring service communication 
while being decoupled. The preference can further be owed to features like flexible 
network models, automatic service discovery, security, isolation, and dynamic scaling.

12. What is the difference between Docker run and Docker start?
Docker’s run command creates and starts a new container from a specific image. Running 
here is done for an already started container that requires the command to execute the 
main process.

The start command works to start a stopped container, which requires beginning to execute 
the main process. The container may have stopped for various reasons, such as using the 
stop command, memory overconsumption, or the operating system.

13. What are Docker volumes?
Docker volumes are the container storage and management components of Docker. It can be 
easily created through the command ‘docker volume create’ or via Docker itself during 
service or container creation.

Docker volumes are preferred because they are easy to manage through Docker CLI commands 
or Docker API, secure sharing across multiple containers, and operability on both Linux and 
Windows containers.

14. How do you update an existing Docker image?
Updating the existing Docker image is possible through the following steps:

Check the current version by using the command ‘docker images’
Pull the latest Docker image by downloading the new version of the image
Use the ‘latest’  tag if unsure about the host system default
You can also update the image to a specific version using the command 
‘docker pull [image]:[version-number]’
Stop and remove the container based on the old image. It requires finding the outdated 
container with the command ‘docker ps’
Stop the container using the command ‘docker stop [container-id-or-name] ’ and 
remove it with the command ‘docker rm [container-id-or-name]’
Create a new container using the run command, which is 
‘docker run [options] [image] [commands]’
Note: You can check whether a container will run the new Docker image by listing the 
containers using the above commands.

15. What is a multi-stage build in Docker?
Multi-stage Docker optimizes the Docker files and images without compromising the ease 
of readability and maintenance. It converts the image-building process into multiple 
stages, which allows for the inclusion of the necessary dependencies per the final 
application's functionality. It offers memory and time efficiency.

16. How can you improve the performance of Docker containers?
Improving the performance of Docker containers is possible in the following ways:

Modify the frequency and size of Docker images when running a Docker container
Run containers as non-root users whenever possible to reduce attacks
Make use of multi-stage build, which is to be done by using two or more ‘FROM’ statements
Set resource containers to limit container resources by using the ‘-- cups’ and ‘-- memory’ flags
Preload the image cache and use lightweight base images
Use monitoring tools to optimize container health
17. How do you share data between containers?
Follow the mentioned steps to share data between containers:

Create an independent volume named Datavolume1 using the command ‘docker volume create --name DataVolume1’
Create a new container and attach Datavolume1 using the command ‘docker run --rm -ti -v DataVolume1:/datavolume1 ubuntu’
Verify its persistence using the command ‘cat /datavolume1/Example1.txt’
Use the command to create a new container and new volume ‘docker run -ti --name=Container2 -v DataVolume2:/datavolume2 ubuntu’
Add data to the volume with the command ‘echo ‘Example2" > /datavolume2/Example2.txt
cat /datavolume2/Example2.txt’
Exit the container and restart. Then verify the volume has mounted and data’s persistence using the command ‘cat /datavolume2/Example2.txt’
Remove the container using ‘docker rm d0d2233b668eddad4986313c7a4a1bc0d2edaf0c7e1c02a6a6256de27db17a63’
Check the persistence by listing the volumes with the command ‘docker volume ls’
Use ‘docker volume rm DataVolume2’ to remove the second volume
Add the data image with the command ‘docker run -ti --rm -v DataVolume3:/var ubuntu’ and then exit and check the volume contents
Create container4 and datavolume4 using the previous command
Now create a file with the text ‘echo "This file is shared between containers" > /datavolume4/Example4.txt’ followed by exiting the container 
Again, create container5 to mount the volume from container4 and check the data persistence
Append the text from container5 and exit the container
Restart container4 with the command ‘docker start -ai Container4’ and check the changes using ‘lcat /datavolume4/Example4.txt’ and exit
Make the volume read-only using ‘docker run -ti --name=Container6 --volumes-from Container4:ro ubuntu’
Check the status by removing the example file using the command ‘rm /datavolume4/Example4.txt’
Exit and clean the containers and volume 

18. How does Docker handle container orchestration?
In addition to other container orchestration tools, Docker can perform the function with 
its built-in tool, Docker Swarm. It is a user-friendly and simple way to orchestrate, 
requiring interactions similar to regular Docker CLI commands.

Swarm allows service discovery, replica count, declarative scaling, and rolling updates. 
Docker Compose is another widely used method of handling container orchestration.

19. What is the use of docker-compose.yml?
Docker-compose.yml is used for container orchestration. It defines the required images, 
exposed ports,  access to host filesystems, and other configurations to run multi-counter 
applications efficiently.

20. How would you manage multiple containers using Docker Compose?
Docker Compose eases the management of multiple containers by defining the entire 
multi-container application in a single YAML file named compose.yml. It allows for 
the running of containers in a specific order and the management of network connections 
easily. It also enables scaling individual services up or down based on demand.

Advanced Docker Interview Questions
The Docker interview questions for experienced professionals are as follows:

21. How does Docker handle orchestration, and what tools are available for it?
Docker can handle orchestration through both built-in tools and external tools. 
The built–in tools include Docker Swarm and Docker Compose. The commonly used external 
tool for orchestration is Kubernetes.

22. What is Docker content trust, and how is it used?
Docker content trust allows verification of the pulled or deployed images from the 
registry server. It verifies the publisher and integrity of the received data and offers 
the ability to use digital signatures for sent and received data. The digital signatures 
are added by the publishers, like organizations or individuals, and signed and stored on 
the Docker Notary server of choice.

23. Explain the concept of namespaces in Docker.
Namespaces are an important concept in Docker that provides an isolated workspace for the 
container. Dockers create a set of namespaces for the container being run. They are 
features of the Docker Linux kernel that isolate kernel resources, allowing each set 
of processes to see only their environment.

There are various types of namespaces, including user namespace, network namespace, 
process ID (PID) namespace, interprocess communication (IPC) namespace, and others.

24. How does Docker achieve container isolation?
Container isolation in Docker is possible through Linux features such as control groups (
cgroups), kernel namespaces, and secure computing mode (seccomp). For instance, Docker 
configures cgroups by creating a new cgroup hierarchy and assigns them the container’s 
processes, Kernel namespaces allow a set of processes an isolated view of resources, 
and seccomp works by allowing filtering the system calls made by a process.

25. What is the role of cgroups in Docker?
Control groups, or cgroups, are a feature of the Linux kernel used to limit resource 
availability and allocate system resources such as memory and CPU for Docker containers. 
They help avoid ‘noisy neighbour’ issues and are used by other containerization tools. 
Docker uses cgroups for container isolation, and each container is associated with its own 
cgroups.

26. How do you configure persistent storage in Docker?
Docker ensures data resilience through container volumes. It involves creating a volume 
and mounting it into the container with a command. The volume remains accessible even after 
deleting the container. The volumes for persistent storage comprise their lifecycle and can 
expand depending on the type of application and data being used.

27. What is a Docker registry mirror, and why would you use it?
Registry mirror refers to the local duplicate of the public registry, which mirrors or 
replicates its file structure.  It pulls the image from the public registry and stores 
it before providing it to the user. Then the next request involves producing the image 
from the legal registry mirror.

The Docker registry mirror protects against public registry outages, performs image 
vulnerability scanning, accelerates pod creation, and avoids limits on accessing the 
public registry.

28. What is the Docker security scan, and how does it work?
Docker security scans assess vulnerabilities in container images. Depending on the scanning 
tool, they can detect a variety of risks. The scanning evaluates the packages and other 
dependencies defined in the container image file to identify vulnerabilities.

Docker security scans alert the user if a vulnerability is found, which can be further 
handled by updating the Docker image to the latest version. Docker generally has built-in 
scanning tools inside the registry, while image scanners like Clair and Anchore can also 
be used for scanning.

29. How can Docker improve Continuous Delivery (CD)?
Docker contributes to better Continuous Delivery (CD) by streamlining and accelerating 
the deployment process. It integrates well with source control management and the integration 
tool. The feature also automatically creates an image when the user submits the code.

Docker also allows developers to build and test applications in parallel, thus reducing 
deployment time while minimizing errors. 

30. What are Docker Plugins, and how are they used?
Docker Plugins are extensions that enhance the capabilities of the Docker engine. There are 
specific types of Docker Plugins, such as volume, authorization, and network plugins. 
The volume allows volume persistence across various Docker hosts, while the network plugin 
allows network plumbing. The authorization plugin handles Docker daemon requests.
"""